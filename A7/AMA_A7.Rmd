---
title: "Smoothing and regression splines"
author: "Biel Caballero Verg√©s, Svenja Menzenbach and Kleber Enrique Reyes Illescas"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading libraries, include=FALSE}
library(sm)
library(mgcv)
```

# Hirsutism dataset

## GAMs for hirsutism data
```{r}
hirs <- read.table("hirsutism.dat",header=T, sep="\t",fill=TRUE)

#summary(hirs)
attach(hirs)
```
```{r}
plot(hirs[,-c(1,3,4)])
```

```{r}
old.par<-par(mfrow=c(2,3))
for (j in c(2,6,7,8,9)) hist(hirs[,j],main=names(hirs)[j])
par(old.par)
```

```{r}
apply(hirs[,-c(1,3,4,5)],2,sd, na.rm = T) # sd: standard deviation
```

```{r}
apply(hirs[,-c(1,3,4,5)],2,function(x){diff(range(x, na.rm = T))})
```

There isn't a clear way to make subgroups (based on dispersion). It's important to note that SysPres and DiaPres appear highly correlated (both variables are related with blood pressure).

At first we will build a linear model including all variables.
```{r}
# multiple linear regression
gam_0.1 <- gam(FGm12 ~ FGm0 + SysPres + DiaPres + height + weight + Treatment)
summary(gam_0.1)
```
```{r}
plot(gam_0.1, all.terms=TRUE)
```
All variables show a nearly constant partial which concludes to a bad model as no variable seems to have an effect on the outcome.

Next we will try a model with only smoothing functions and using Treatment as a factor.

```{r}
# generative additive model
gam_1.1 <- gam(FGm12 ~ s(FGm0, by=Treatment) + s(SysPres, by=Treatment) + s(DiaPres, by=Treatment) + s(height, by=Treatment) + s(weight, by=Treatment))
summary(gam_1.1)
```

```{r}
plot(gam_1.1,residuals=TRUE, shade=TRUE, shade.col="lightblue", seWithMean=TRUE, cex=3, lwd=2, shift= coef(gam_1.1)[1])
```
```{r}
gam.check(gam_1.1)
```
The partial plots are still quite constant. Hence we didn't find a good model yet. However, the deviance explained, giving insight about the quality of the fit, is more than twice as high as in the previous models.


We also create a model like gam_1.1 but without Treatment as factor, to see what difference it makes.

```{r} 
gam_1.2 <- gam(FGm12 ~ s(FGm0) + s(SysPres) + s(DiaPres) + s(height) + s(weight) )
summary(gam_1.2)
```


```{r}
plot(gam_1.2,residuals=TRUE, shade=TRUE, shade.col="lightblue", seWithMean=TRUE, cex=3, lwd=2, shift= coef(gam_1.1)[1])
```
```{r}
gam.check(gam_1.2)
```

Now the partials show us less constant plots, but the deviance explained is less than using treatment as factor, letting us conclude that it is good too use Treatment as factor.


Further, we will create semiparametric models. We look at the effective degree of freedoms of model 1.1.  SysPres, FGm0 and DiaPres have a edf close to 1 indicating a possible linear relationship to our prediction. It might improve the model to simplify it and incorporate this variables as linear terms. At first we set each of them linear respectively while letting the other parameters wrapped by a smoothing function.

```{r}
# semiparametric model - SysPres (low df at model 1.1)
gam_2.1 <- gam(FGm12 ~ s(FGm0, by=Treatment) + SysPres + s(DiaPres, by=Treatment) + s(height, by=Treatment) + s(weight, by=Treatment))
summary(gam_2.1)
```
```{r}
plot(gam_2.1,residuals=TRUE, shade=TRUE, shade.col="lightblue", seWithMean=TRUE, cex=3, lwd=2, shift= coef(gam_2.1)[1])
```
```{r}
gam.check(gam_2.1)
```
```{r}
# semiparametric model - FGm0 (low df at model 1.1)
gam_2.2 <- gam(FGm12 ~ FGm0 + s(SysPres, by=Treatment) + s(DiaPres, by=Treatment) + s(height, by=Treatment) + s(weight, by=Treatment))
summary(gam_2.2)
```
```{r}
plot(gam_2.2,residuals=TRUE, shade=TRUE, shade.col="lightblue", seWithMean=TRUE, cex=3, lwd=2, shift= coef(gam_2.2)[1])
```
```{r}
gam.check(gam_2.2)
```
```{r}
# semiparametric model - DiaPres (low df at model 1.1)
gam_2.3 <- gam(FGm12 ~ s(FGm0, by=Treatment) + s(SysPres, by=Treatment) + DiaPres + s(height, by=Treatment) + s(weight, by=Treatment))
summary(gam_2.3)
```
```{r}
plot(gam_2.3,residuals=TRUE, shade=TRUE, shade.col="lightblue", seWithMean=TRUE, cex=3, lwd=2, shift= coef(gam_2.3)[1])
```
```{r}
gam.check(gam_2.3)
```

We also set two of them linear:
```{r}
# semiparametric model - FGmO + SysPres
gam_3.1 <- gam(FGm12 ~ FGm0 + SysPres + s(DiaPres, by=Treatment) + s(height, by=Treatment) + s(weight, by=Treatment))
summary(gam_3.1)
```
```{r}
plot(gam_3.1,residuals=TRUE, shade=TRUE, shade.col="lightblue", seWithMean=TRUE, cex=3, lwd=2, shift= coef(gam_3.1)[1])
```
```{r}
gam.check(gam_3.1)
```

```{r}
# semiparametric model - FGmO + DiaPres
gam_3.2 <- gam(FGm12 ~ FGm0 + s(SysPres, by=Treatment) + DiaPres + s(height, by=Treatment) + s(weight, by=Treatment))
summary(gam_3.2)
```
```{r}
plot(gam_3.2,residuals=TRUE, shade=TRUE, shade.col="lightblue", seWithMean=TRUE, cex=3, lwd=2, shift= coef(gam_3.2)[1])
```
```{r}
gam.check(gam_3.2)
```

Here we removed the factor from weight.
```{r}
gam_4.1 <- gam(FGm12 ~ FGm0 + SysPres + s(DiaPres, by=Treatment) + s(height, by=Treatment) + s(weight))
summary(gam_4.1)
```
```{r}
plot(gam_4.1,residuals=TRUE, shade=TRUE, shade.col="lightblue", seWithMean=TRUE, cex=3, lwd=2, shift= coef(gam_4.1)[1])
```
```{r}
gam.check(gam_4.1)
```
Here we tried a new approach and combined variables in a smoothing function. Also we removed DiaPress which is reasonable as it is linear correlated looking at the correlation plot at the beginning of the file.
```{r}
gam_5.1 <- gam(FGm12 ~ SysPres +  s(FGm0,SysPres, by=Treatment, k=5) + s(height, k=25, bs="cr") + s(weight,fx=TRUE,bs="cr"))
summary(gam_5.1)
```
Since the interaction is taking into account the different factors from Treatment we had a lot of uncertainty
```{r}
plot(gam_5.1,residuals=TRUE, shade=TRUE, shade.col="lightblue", seWithMean=TRUE, cex=3, lwd=2, shift= coef(gam_5.1)[1])
```
This model has a comparably high value for deviance explained and the partial plots are not constant as for many previous models. R-sq is also higher.

```{r}
vis.gam(gam_5.1, view = c("FGm0","SysPres"), plot.type = "persp", se=2, 
            theta = 140,  phi=30)
```

```{r}
gam.check(gam_5.1)
```
These latest plots look better than the previous ones. However, the Q-Q plot is not straight line, and the histogram lacks a well-defined bell shape, implying that the model isn't perfect.

Looking at the console result, the model converged without issues, and the p-values from the smooths are considerably high. This suggests that we have a good number of basis functions (k) to be confident in our model

## Anova tests

```{r}
anova(gam_0.1,gam_1.1,test="F")
```
Gam_1.1 better
```{r}
anova(gam_0.1,gam_1.2,test="F")
```
Gam_1.2 better
```{r}
anova(gam_1.2,gam_1.1,test="F")
```
Gam_1.1 better (with treatment)

```{r}
anova(gam_2.1,gam_2.2,test="F")
```

Gam_2.2 better

```{r}
anova(gam_2.2,gam_2.3,test="F")
```
Gam_2.2 better

```{r}
anova(gam_2.2,gam_1.1,test="F")
```
Gam_1.1 better

```{r}
anova(gam_3.1,gam_3.2,test="F")
```
Gam_3.1 better

```{r}
anova(gam_3.1,gam_1.1,test="F")
```
Gam_3.1 better


```{r}
anova(gam_3.1,gam_4.1,test="F")
```

Gam_4.1 better

```{r}
anova(gam_4.1, gam_5.1,test="F")
```

Gam_5.1 better

Comparing the models with anova, the model gam_5.1 is the best obtained model out of them.


## Compare plot of a "bad" and a "good" model
```{r}
vis.gam(gam_0.1,view=c("FGm0","weight"),
        theta = 40, phi = 25, r = sqrt(3), d = 1)
```
```{r}
vis.gam(gam_3.1,view=c("FGm0","weight"),
        theta = 40, phi = 25, r = sqrt(3), d = 1)
```


```{r}
vis.gam(gam_5.1,view=c("FGm0","weight"),
        theta = 40, phi = 25, r = sqrt(3), d = 1)
```
We can see that the better performing model is more complex. The complexity might be necessary to represent the relation of the variables to our prediction value properly. 








